{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3bbcc9-9413-40bf-b6ad-75b0cee3a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import polars as pl\n",
    "from utils2 import *\n",
    "\n",
    "# Configuración de logging\n",
    "logger = configure_logging()\n",
    "\n",
    "# Directorio para guardar los checkpoints\n",
    "checkpoint_dir = create_checkpoint_dir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8a05d7-8fff-45b3-abef-7da5e88696e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo de datos\n",
    "data_filepath = \"data/diabetic_data.csv\"\n",
    "\n",
    "# Columnas categóricas a convertir\n",
    "categorical_cols = [\n",
    "    'race', 'gender', 'age', 'payer_code', 'medical_specialty', \n",
    "    'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', \n",
    "    'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', \n",
    "    'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', \n",
    "    'tolazamide', 'examide', 'citoglipton', 'insulin', \n",
    "    'glyburide-metformin', 'glipizide-metformin', \n",
    "    'glimepiride-pioglitazone', 'metformin-rosiglitazone', \n",
    "    'metformin-pioglitazone', 'change', 'diabetesMed', \n",
    "    'readmitted'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9283f93c-d75a-4c70-90eb-f3b37ebd55c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:28:28,931 - INFO - Loading data\n",
      "2024-06-27 08:28:28,979 - INFO - Handling missing values\n",
      "2024-06-27 08:28:28,985 - INFO - Converting categorical columns to numerical\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de datos\n",
    "df = preprocess_data(data_filepath, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ec9b7b-15a3-4118-9e0a-4dbed2baeebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:28:29,078 - INFO - Selecting features and target\n"
     ]
    }
   ],
   "source": [
    "# Selección de características y objetivo\n",
    "logger.info(\"Selecting features and target\")\n",
    "features = df.select([\n",
    "    'encounter_id', 'patient_nbr', 'race', 'gender', 'age', \n",
    "    'admission_type_id', 'discharge_disposition_id', 'admission_source_id', \n",
    "    'time_in_hospital', 'num_lab_procedures', 'num_procedures', \n",
    "    'num_medications'\n",
    "])\n",
    "target = df.select('readmitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb40c934-549a-4bb1-8693-e597de22cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:28:29,086 - INFO - Converting data to PyTorch tensors\n",
      "2024-06-27 08:28:29,095 - INFO - X_tensor shape: torch.Size([101766, 12]), y_tensor shape: torch.Size([101766])\n",
      "2024-06-27 08:28:29,102 - INFO - Sample y_tensor values: tensor([0., 1., 2.])\n",
      "2024-06-27 08:28:29,102 - INFO - X_np shape: (101766, 12), y_np shape: (101766,)\n"
     ]
    }
   ],
   "source": [
    "# Convertir a tensores de PyTorch\n",
    "logger.info(\"Converting data to PyTorch tensors\")\n",
    "X_tensor = torch.tensor(features.to_numpy(), dtype=torch.float32)\n",
    "y_tensor = torch.tensor(target.to_numpy(), dtype=torch.float32).view(-1)\n",
    "\n",
    "# Convertir a arrays de NumPy\n",
    "X_np = features.to_numpy()\n",
    "y_np = target.to_numpy().flatten()\n",
    "\n",
    "# Verificar las dimensiones y los valores de los tensores y arrays\n",
    "logger.info(f\"X_tensor shape: {X_tensor.shape}, y_tensor shape: {y_tensor.shape}\")\n",
    "logger.info(f\"Sample y_tensor values: {y_tensor.unique()}\")\n",
    "logger.info(f\"X_np shape: {X_np.shape}, y_np shape: {y_np.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea3eae25-eaa0-4f7b-bd49-4af01e8756ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:28:29,108 - INFO - Creating DataLoader\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataLoader\n",
    "logger.info(\"Creating DataLoader\")\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b915f9f6-c22b-4274-8614-2d6b96c5cbe8",
   "metadata": {},
   "source": [
    "# MNN (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5486a7c3-78f1-470c-b12f-75b61fd1444e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:28:29,137 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo\n",
    "input_dim = X_tensor.shape[1]\n",
    "output_dim = 3  # Para 3 clases\n",
    "model = SimpleNN(input_dim, output_dim)\n",
    "\n",
    "# Entrenar el modelo en GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Cambiado a CrossEntropyLoss para multiclase\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aceb5704-9c8b-493a-8ae4-8aeeee8c875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:28:30,857 - INFO - Starting training\n",
      "2024-06-27 08:28:37,871 - INFO - Epoch 1/10, Loss: 102822.1074\n",
      "2024-06-27 08:28:37,877 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_1.pth\n",
      "2024-06-27 08:28:44,934 - INFO - Epoch 2/10, Loss: 9179.1247\n",
      "2024-06-27 08:28:44,939 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_2.pth\n",
      "2024-06-27 08:28:52,041 - INFO - Epoch 3/10, Loss: 0.9466\n",
      "2024-06-27 08:28:52,047 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_3.pth\n",
      "2024-06-27 08:28:59,092 - INFO - Epoch 4/10, Loss: 0.9452\n",
      "2024-06-27 08:28:59,097 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_4.pth\n",
      "2024-06-27 08:29:06,310 - INFO - Epoch 5/10, Loss: 0.9452\n",
      "2024-06-27 08:29:06,314 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_5.pth\n",
      "2024-06-27 08:29:13,283 - INFO - Epoch 6/10, Loss: 0.9453\n",
      "2024-06-27 08:29:13,288 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_6.pth\n",
      "2024-06-27 08:29:20,486 - INFO - Epoch 7/10, Loss: 0.9453\n",
      "2024-06-27 08:29:20,491 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_7.pth\n",
      "2024-06-27 08:29:27,611 - INFO - Epoch 8/10, Loss: 0.9452\n",
      "2024-06-27 08:29:27,615 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_8.pth\n",
      "2024-06-27 08:29:34,654 - INFO - Epoch 9/10, Loss: 0.9452\n",
      "2024-06-27 08:29:34,659 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_9.pth\n",
      "2024-06-27 08:29:41,691 - INFO - Epoch 10/10, Loss: 0.9453\n",
      "2024-06-27 08:29:41,696 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_10.pth\n",
      "2024-06-27 08:29:41,696 - INFO - Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "num_epochs = 10\n",
    "logger.info(\"Starting training\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Verificación de tipos y dimensiones\n",
    "        #print(f'outputs shape: {outputs.shape}, labels shape: {labels.shape}')\n",
    "        #print(f'outputs dtype: {outputs.dtype}, labels dtype: {labels.dtype}')\n",
    "        \n",
    "        # Asegurarse de que las etiquetas sean del tipo correcto\n",
    "        labels = labels.long()\n",
    "        \n",
    "        loss = criterion(outputs, labels)  # CrossEntropyLoss espera que las etiquetas sean torch.long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    logger.info(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "    # Guardar checkpoint al final de cada epoch\n",
    "    save_checkpoint(epoch, model, optimizer, avg_epoch_loss, checkpoint_dir)\n",
    "\n",
    "logger.info(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b95282-fd19-4bb2-8b98-7fb6c4a39ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:29:41,704 - INFO - Evaluating the model\n",
      "2024-06-27 08:29:44,799 - INFO - Model Accuracy: 0.5391191557101586\n",
      "2024-06-27 08:29:44,800 - INFO - Model Precision: 0.7515303083434756\n",
      "2024-06-27 08:29:44,801 - INFO - Model Recall: 0.5391191557101586\n",
      "2024-06-27 08:29:44,802 - INFO - Model F1 Score: 0.37768286227264436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluación del modelo\n",
    "logger.info(\"Evaluating the model\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)  # Obtener el índice de la clase con mayor probabilidad\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy_torch = accuracy_score(all_labels, all_preds)\n",
    "precision_torch = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "recall_torch = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1_torch = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "logger.info(f\"Model Accuracy: {accuracy_torch}\")\n",
    "logger.info(f\"Model Precision: {precision_torch}\")\n",
    "logger.info(f\"Model Recall: {recall_torch}\")\n",
    "logger.info(f\"Model F1 Score: {f1_torch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14a439-fe46-410f-8c6c-39f0ebb41cd4",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d57aac6-f92d-48a6-ad91-9b951766689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos y el modelo\n",
    "input_dim = X_tensor.shape[1]\n",
    "output_dim = 3  # Para 3 clases\n",
    "model = CNN(input_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Crear DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "038cc107-859e-49bb-a598-148062ab840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:29:44,823 - INFO - Starting training with CNN\n",
      "2024-06-27 08:29:54,674 - INFO - Epoch 1/10, Loss: 17923.1220\n",
      "2024-06-27 08:29:54,680 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_1.pth\n",
      "2024-06-27 08:30:04,481 - INFO - Epoch 2/10, Loss: 200.8769\n",
      "2024-06-27 08:30:04,486 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_2.pth\n",
      "2024-06-27 08:30:14,302 - INFO - Epoch 3/10, Loss: 0.9366\n",
      "2024-06-27 08:30:14,309 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_3.pth\n",
      "2024-06-27 08:30:24,074 - INFO - Epoch 4/10, Loss: 0.9338\n",
      "2024-06-27 08:30:24,081 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_4.pth\n",
      "2024-06-27 08:30:33,905 - INFO - Epoch 5/10, Loss: 1.1471\n",
      "2024-06-27 08:30:33,911 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_5.pth\n",
      "2024-06-27 08:30:43,641 - INFO - Epoch 6/10, Loss: 1.0380\n",
      "2024-06-27 08:30:43,648 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_6.pth\n",
      "2024-06-27 08:30:53,424 - INFO - Epoch 7/10, Loss: 16.3760\n",
      "2024-06-27 08:30:53,430 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_7.pth\n",
      "2024-06-27 08:31:03,846 - INFO - Epoch 8/10, Loss: 1.2252\n",
      "2024-06-27 08:31:03,853 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_8.pth\n",
      "2024-06-27 08:31:13,773 - INFO - Epoch 9/10, Loss: 1.2814\n",
      "2024-06-27 08:31:13,780 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_9.pth\n",
      "2024-06-27 08:31:23,522 - INFO - Epoch 10/10, Loss: 1.3712\n",
      "2024-06-27 08:31:23,528 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_10.pth\n",
      "2024-06-27 08:31:23,529 - INFO - Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "num_epochs = 10\n",
    "logger.info(\"Starting training with CNN\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Verificación de tipos y dimensiones\n",
    "        #print(f'outputs shape: {outputs.shape}, labels shape: {labels.shape}')\n",
    "        #print(f'outputs dtype: {outputs.dtype}, labels dtype: {labels.dtype}')\n",
    "        \n",
    "        # Asegurarse de que las etiquetas sean del tipo correcto\n",
    "        labels = labels.long()\n",
    "        \n",
    "        loss = criterion(outputs, labels)  # CrossEntropyLoss espera que las etiquetas sean torch.long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    logger.info(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "    # Guardar checkpoint al final de cada epoch\n",
    "    save_checkpoint(epoch, model, optimizer, avg_epoch_loss, checkpoint_dir)\n",
    "\n",
    "logger.info(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ab706db-6dae-428c-b1cd-0d93e7b6727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:31:23,540 - INFO - Evaluating the CNN model\n",
      "2024-06-27 08:31:27,613 - INFO - CNN Accuracy: 0.5399544052040957\n",
      "2024-06-27 08:31:27,613 - INFO - CNN Precision: 0.5635968739096631\n",
      "2024-06-27 08:31:27,614 - INFO - CNN Recall: 0.5399544052040957\n",
      "2024-06-27 08:31:27,615 - INFO - CNN F1 Score: 0.4014093274978078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluación del modelo\n",
    "logger.info(\"Evaluating the CNN model\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)  # Obtener el índice de la clase con mayor probabilidad\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy_cnn = accuracy_score(all_labels, all_preds)\n",
    "precision_cnn = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "recall_cnn = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1_cnn = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "logger.info(f\"CNN Accuracy: {accuracy_cnn}\")\n",
    "logger.info(f\"CNN Precision: {precision_cnn}\")\n",
    "logger.info(f\"CNN Recall: {recall_cnn}\")\n",
    "logger.info(f\"CNN F1 Score: {f1_cnn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f2726-3b51-42c0-937a-a465a15ef630",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28735ddd-4375-466d-b6d6-015e9147e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que X_tensor tenga una dimensión de secuencia de longitud 1\n",
    "# Crear una copia de X_tensor para el RNN\n",
    "X_tensor_rnn = X_tensor.clone().unsqueeze(1)\n",
    "\n",
    "# Preparar los datos y el modelo\n",
    "input_dim = X_tensor_rnn.shape[2]\n",
    "hidden_dim = 128\n",
    "output_dim = 3  # Para 3 clases\n",
    "num_layers = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RNN(input_dim, hidden_dim, output_dim, num_layers, device).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Crear DataLoader\n",
    "dataset = TensorDataset(X_tensor_rnn, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9bda7c-0943-4d3c-9073-70e8cb252ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:31:27,644 - INFO - Starting training with RNN\n",
      "2024-06-27 08:31:39,665 - INFO - Epoch 1/10, Loss: 0.9392\n",
      "2024-06-27 08:31:39,675 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_1.pth\n",
      "2024-06-27 08:31:51,392 - INFO - Epoch 2/10, Loss: 0.9379\n",
      "2024-06-27 08:31:51,401 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_2.pth\n",
      "2024-06-27 08:32:03,115 - INFO - Epoch 3/10, Loss: 0.9377\n",
      "2024-06-27 08:32:03,125 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_3.pth\n",
      "2024-06-27 08:32:14,825 - INFO - Epoch 4/10, Loss: 0.9373\n",
      "2024-06-27 08:32:14,834 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_4.pth\n",
      "2024-06-27 08:32:26,651 - INFO - Epoch 5/10, Loss: 0.9368\n",
      "2024-06-27 08:32:26,659 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_5.pth\n",
      "2024-06-27 08:32:38,511 - INFO - Epoch 6/10, Loss: 0.9367\n",
      "2024-06-27 08:32:38,521 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_6.pth\n",
      "2024-06-27 08:32:50,147 - INFO - Epoch 7/10, Loss: 0.9366\n",
      "2024-06-27 08:32:50,156 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_7.pth\n",
      "2024-06-27 08:33:02,005 - INFO - Epoch 8/10, Loss: 0.9367\n",
      "2024-06-27 08:33:02,014 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_8.pth\n",
      "2024-06-27 08:33:14,027 - INFO - Epoch 9/10, Loss: 0.9365\n",
      "2024-06-27 08:33:14,036 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_9.pth\n",
      "2024-06-27 08:33:25,751 - INFO - Epoch 10/10, Loss: 0.9365\n",
      "2024-06-27 08:33:25,760 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_10.pth\n",
      "2024-06-27 08:33:25,761 - INFO - Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "num_epochs = 10\n",
    "logger.info(\"Starting training with RNN\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Verificación de tipos y dimensiones\n",
    "        #print(f'outputs shape: {outputs.shape}, labels shape: {labels.shape}')\n",
    "        #print(f'outputs dtype: {outputs.dtype}, labels dtype: {labels.dtype}')\n",
    "        \n",
    "        # Asegurarse de que las etiquetas sean del tipo correcto\n",
    "        labels = labels.long()\n",
    "        \n",
    "        loss = criterion(outputs, labels)  # CrossEntropyLoss espera que las etiquetas sean torch.long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    logger.info(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "    # Guardar checkpoint al final de cada epoch\n",
    "    save_checkpoint(epoch, model, optimizer, avg_epoch_loss, checkpoint_dir)\n",
    "\n",
    "logger.info(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80dfb4ae-ff6e-4630-8e36-23c33bb79763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:33:25,769 - INFO - Evaluating the RNN model\n",
      "2024-06-27 08:33:30,344 - INFO - RNN Accuracy: 0.5431774856042293\n",
      "2024-06-27 08:33:30,345 - INFO - RNN Precision: 0.5699439045010093\n",
      "2024-06-27 08:33:30,345 - INFO - RNN Recall: 0.5431774856042293\n",
      "2024-06-27 08:33:30,346 - INFO - RNN F1 Score: 0.4417652466289006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluación del modelo\n",
    "logger.info(\"Evaluating the RNN model\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)  # Obtener el índice de la clase con mayor probabilidad\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy_rnn = accuracy_score(all_labels, all_preds)\n",
    "precision_rnn = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "recall_rnn = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1_rnn = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "logger.info(f\"RNN Accuracy: {accuracy_rnn}\")\n",
    "logger.info(f\"RNN Precision: {precision_rnn}\")\n",
    "logger.info(f\"RNN Recall: {recall_rnn}\")\n",
    "logger.info(f\"RNN F1 Score: {f1_rnn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabe35a-066c-4e2a-aec7-163b799b5af2",
   "metadata": {},
   "source": [
    "# Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6fa3530-ff18-4876-ba0f-4965e669b6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:33:30,826 - INFO - Logistic Regression Accuracy: 0.5383217058072124\n",
      "2024-06-27 08:33:30,827 - INFO - Logistic Regression Precision: 0.5620881913822836\n",
      "2024-06-27 08:33:30,827 - INFO - Logistic Regression Recall: 0.5383217058072124\n",
      "2024-06-27 08:33:30,828 - INFO - Logistic Regression F1 Score: 0.44532293893748903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de regresión logística\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y evaluar el modelo\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "logger.info(f\"Logistic Regression Accuracy: {accuracy}\")\n",
    "logger.info(f\"Logistic Regression Precision: {precision}\")\n",
    "logger.info(f\"Logistic Regression Recall: {recall}\")\n",
    "logger.info(f\"Logistic Regression F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551c4f1-b4b9-4f9e-9fe3-3fb44213c550",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e77cf29-2621-4130-9326-53898fdf986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:33:49,159 - INFO - Random Forest Accuracy: 0.5703055910386164\n",
      "2024-06-27 08:33:49,160 - INFO - Random Forest Precision: 0.5416231091212172\n",
      "2024-06-27 08:33:49,162 - INFO - Random Forest Recall: 0.5703055910386164\n",
      "2024-06-27 08:33:49,163 - INFO - Random Forest F1 Score: 0.5313892426143688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Crear y entrenar el modelo de bosques aleatorios\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y evaluar el modelo\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted', zero_division=1)\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "logger.info(f\"Random Forest Accuracy: {accuracy_rf}\")\n",
    "logger.info(f\"Random Forest Precision: {precision_rf}\")\n",
    "logger.info(f\"Random Forest Recall: {recall_rf}\")\n",
    "logger.info(f\"Random Forest F1 Score: {f1_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ef15d-a05f-4621-b06b-3b494dd79732",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8258ad9-bb9e-4ddf-819f-b5e0d388257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:36:32,851 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo\n",
    "input_dim = X_tensor.shape[1]\n",
    "output_dim = 3  # Para 3 clases\n",
    "model = SVM(input_dim)\n",
    "\n",
    "# Entrenar el modelo en GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e7be4ed-9faa-4f0e-9ae4-577ca477eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:38:53,971 - INFO - Starting training with SVM\n",
      "2024-06-27 08:38:58,497 - INFO - Epoch 1/10, Loss: 83059345525015.0312\n",
      "2024-06-27 08:38:58,500 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_1.pth\n",
      "2024-06-27 08:39:03,063 - INFO - Epoch 2/10, Loss: 81849635417783.6562\n",
      "2024-06-27 08:39:03,065 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_2.pth\n",
      "2024-06-27 08:39:07,505 - INFO - Epoch 3/10, Loss: 81515527819056.6875\n",
      "2024-06-27 08:39:07,507 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_3.pth\n",
      "2024-06-27 08:39:11,862 - INFO - Epoch 4/10, Loss: 82220631610001.9844\n",
      "2024-06-27 08:39:11,864 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_4.pth\n",
      "2024-06-27 08:39:16,409 - INFO - Epoch 5/10, Loss: 82259972548499.1875\n",
      "2024-06-27 08:39:16,411 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_5.pth\n",
      "2024-06-27 08:39:20,809 - INFO - Epoch 6/10, Loss: 83299440152492.2969\n",
      "2024-06-27 08:39:20,811 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_6.pth\n",
      "2024-06-27 08:39:25,211 - INFO - Epoch 7/10, Loss: 82283683871500.9531\n",
      "2024-06-27 08:39:25,213 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_7.pth\n",
      "2024-06-27 08:39:29,667 - INFO - Epoch 8/10, Loss: 82416095233596.6875\n",
      "2024-06-27 08:39:29,669 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_8.pth\n",
      "2024-06-27 08:39:34,078 - INFO - Epoch 9/10, Loss: 82349333630912.9062\n",
      "2024-06-27 08:39:34,080 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_9.pth\n",
      "2024-06-27 08:39:38,438 - INFO - Epoch 10/10, Loss: 82385101525964.1719\n",
      "2024-06-27 08:39:38,440 - INFO - Checkpoint saved: checkpoints\\checkpoint_epoch_10.pth\n",
      "2024-06-27 08:39:38,441 - INFO - Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "num_epochs = 10\n",
    "logger.info(\"Starting training with SVM\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Asegurarse de que las salidas tengan el tamaño correcto\n",
    "        outputs = outputs.squeeze(1)\n",
    "        \n",
    "        # Verificación de tipos y dimensiones\n",
    "        #print(f'outputs shape: {outputs.shape}, labels shape: {labels.shape}')\n",
    "        #print(f'outputs dtype: {outputs.dtype}, labels dtype: {labels.dtype}')\n",
    "        \n",
    "        # Asegurarse de que las etiquetas sean del tipo correcto y tengan el tamaño correcto\n",
    "        labels = labels.squeeze().long()\n",
    "        \n",
    "        loss = criterion(outputs, labels)  # CrossEntropyLoss espera que las etiquetas sean torch.long\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    logger.info(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "    # Guardar checkpoint al final de cada epoch\n",
    "    save_checkpoint(epoch, model, optimizer, avg_epoch_loss, checkpoint_dir)\n",
    "\n",
    "logger.info(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eff585dc-75c3-49ec-a5e7-b6b77b805bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:42:37,738 - INFO - Evaluating the SVM model\n",
      "2024-06-27 08:42:40,217 - INFO - SVM Accuracy: 0.5391191557101586\n",
      "2024-06-27 08:42:40,218 - INFO - SVM Precision: 0.7515303083434756\n",
      "2024-06-27 08:42:40,218 - INFO - SVM Recall: 0.5391191557101586\n",
      "2024-06-27 08:42:40,220 - INFO - SVM F1 Score: 0.37768286227264436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluación del modelo\n",
    "logger.info(\"Evaluating the SVM model\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        _, preds = torch.max(outputs, 1)  # Obtener el índice de la clase con mayor probabilidad\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convertir las listas a arrays de NumPy para asegurar compatibilidad con sklearn\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy_svm = accuracy_score(all_labels, all_preds)\n",
    "precision_svm = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "recall_svm = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1_svm = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "logger.info(f\"SVM Accuracy: {accuracy_svm}\")\n",
    "logger.info(f\"SVM Precision: {precision_svm}\")\n",
    "logger.info(f\"SVM Recall: {recall_svm}\")\n",
    "logger.info(f\"SVM F1 Score: {f1_svm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc99b6-9d0f-4db9-ac11-307647511477",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "773638a7-44d0-4e70-83f2-b7fd6b8aae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:45:08,726 - INFO - K-Nearest Neighbors Accuracy: 0.5156725950673087\n",
      "2024-06-27 08:45:08,728 - INFO - K-Nearest Neighbors Precision: 0.47017378390553505\n",
      "2024-06-27 08:45:08,729 - INFO - K-Nearest Neighbors Recall: 0.5156725950673087\n",
      "2024-06-27 08:45:08,730 - INFO - K-Nearest Neighbors F1 Score: 0.48191514846137073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Crear y entrenar el modelo de K-Vecinos más Cercanos\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y evaluar el modelo\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy_knn = accuracy_score(y_test, y_pred)\n",
    "precision_knn = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "recall_knn = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_knn = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "logger.info(f\"K-Nearest Neighbors Accuracy: {accuracy_knn}\")\n",
    "logger.info(f\"K-Nearest Neighbors Precision: {precision_knn}\")\n",
    "logger.info(f\"K-Nearest Neighbors Recall: {recall_knn}\")\n",
    "logger.info(f\"K-Nearest Neighbors F1 Score: {f1_knn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a850f-de42-4043-a570-533473d12b3d",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b771610a-4f3e-4ef8-a4b6-e41b5339c7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:46:07,464 - INFO - Gradient Boosting Accuracy: 0.5738429792669746\n",
      "2024-06-27 08:46:07,465 - INFO - Gradient Boosting Precision: 0.575850997583986\n",
      "2024-06-27 08:46:07,466 - INFO - Gradient Boosting Recall: 0.5738429792669746\n",
      "2024-06-27 08:46:07,466 - INFO - Gradient Boosting F1 Score: 0.5252324064242889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Crear y entrenar el modelo de Gradient Boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y evaluar el modelo\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy_gb = accuracy_score(y_test, y_pred)\n",
    "precision_gb = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "recall_gb = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_gb = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "logger.info(f\"Gradient Boosting Accuracy: {accuracy_gb}\")\n",
    "logger.info(f\"Gradient Boosting Precision: {precision_gb}\")\n",
    "logger.info(f\"Gradient Boosting Recall: {recall_gb}\")\n",
    "logger.info(f\"Gradient Boosting F1 Score: {f1_gb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35412d-e99c-4c5d-a720-9cb4b0bf6997",
   "metadata": {},
   "source": [
    "# Red Neuronal Multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a6d3052-5909-4e58-99eb-ce1e14810ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:48:47,560 - INFO - MLP Accuracy: 0.5380760538469097\n",
      "2024-06-27 08:48:47,561 - INFO - MLP Precision: 0.7514497858765528\n",
      "2024-06-27 08:48:47,563 - INFO - MLP Recall: 0.5380760538469097\n",
      "2024-06-27 08:48:47,563 - INFO - MLP F1 Score: 0.376477923831301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Crear y entrenar el modelo de Red Neuronal Multicapa\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y evaluar el modelo\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred)\n",
    "precision_mlp = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "recall_mlp = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_mlp = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "logger.info(f\"MLP Accuracy: {accuracy_mlp}\")\n",
    "logger.info(f\"MLP Precision: {precision_mlp}\")\n",
    "logger.info(f\"MLP Recall: {recall_mlp}\")\n",
    "logger.info(f\"MLP F1 Score: {f1_mlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc6699-e533-4c0d-8141-7dfe129c7677",
   "metadata": {},
   "source": [
    "# Arbol de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a6d6dea-270a-42c2-89af-8d1eaad4d78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 08:49:54,863 - INFO - Decision Tree Accuracy: 0.4713569814287118\n",
      "2024-06-27 08:49:54,864 - INFO - Decision Tree Precision: 0.477029116876877\n",
      "2024-06-27 08:49:54,864 - INFO - Decision Tree Recall: 0.4713569814287118\n",
      "2024-06-27 08:49:54,866 - INFO - Decision Tree F1 Score: 0.4741007214725254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Crear y entrenar el modelo de Árbol de Decisión\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y evaluar el modelo\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Calcular las métricas\n",
    "accuracy_dt = accuracy_score(y_test, y_pred)\n",
    "precision_dt = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "recall_dt = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_dt = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "logger.info(f\"Decision Tree Accuracy: {accuracy_dt}\")\n",
    "logger.info(f\"Decision Tree Precision: {precision_dt}\")\n",
    "logger.info(f\"Decision Tree Recall: {recall_dt}\")\n",
    "logger.info(f\"Decision Tree F1 Score: {f1_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa370824-3a1e-41aa-87e1-4479f197687b",
   "metadata": {},
   "source": [
    "# Seleccion del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15d43360-8530-4121-9525-90435f08ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.538322   0.562088  0.538322  0.445323\n",
      "1        Random Forest  0.570306   0.541623  0.570306  0.531389\n",
      "2        SVM (PyTorch)  0.539119   0.751530  0.539119  0.377683\n",
      "3  K-Nearest Neighbors  0.515673   0.470174  0.515673  0.481915\n",
      "4    Gradient Boosting  0.573843   0.575851  0.573843  0.525232\n",
      "5   MLP (scikit-learn)  0.538076   0.751450  0.538076  0.376478\n",
      "6        MLP (PyTorch)  0.539119   0.751530  0.539119  0.377683\n",
      "7        Decision Tree  0.471357   0.477029  0.471357  0.474101\n",
      "8        CNN (PyTorch)  0.539954   0.563597  0.539954  0.401409\n",
      "9        RNN (PyTorch)  0.543177   0.569944  0.543177  0.441765\n",
      "The best model based on the combination of metrics is: Gradient Boosting\n",
      "Model performance comparison:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.538322   0.562088  0.538322  0.445323\n",
      "1        Random Forest  0.570306   0.541623  0.570306  0.531389\n",
      "2        SVM (PyTorch)  0.539119   0.751530  0.539119  0.377683\n",
      "3  K-Nearest Neighbors  0.515673   0.470174  0.515673  0.481915\n",
      "4    Gradient Boosting  0.573843   0.575851  0.573843  0.525232\n",
      "5   MLP (scikit-learn)  0.538076   0.751450  0.538076  0.376478\n",
      "6        MLP (PyTorch)  0.539119   0.751530  0.539119  0.377683\n",
      "7        Decision Tree  0.471357   0.477029  0.471357  0.474101\n",
      "8        CNN (PyTorch)  0.539954   0.563597  0.539954  0.401409\n",
      "9        RNN (PyTorch)  0.543177   0.569944  0.543177  0.441765\n"
     ]
    }
   ],
   "source": [
    "# Suponiendo que tenemos las métricas de varios modelos\n",
    "results = {\n",
    "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"SVM (PyTorch)\", \"K-Nearest Neighbors\", \"Gradient Boosting\", \"MLP (scikit-learn)\", \"MLP (PyTorch)\", \"Decision Tree\", \"CNN (PyTorch)\", \"RNN (PyTorch)\"],\n",
    "    \"Accuracy\": [accuracy, accuracy_rf, accuracy_svm, accuracy_knn, accuracy_gb, accuracy_mlp, accuracy_torch, accuracy_dt, accuracy_cnn, accuracy_rnn],\n",
    "    \"Precision\": [precision, precision_rf, precision_svm, precision_knn, precision_gb, precision_mlp, precision_torch, precision_dt, precision_cnn, precision_rnn],\n",
    "    \"Recall\": [recall, recall_rf, recall_svm, recall_knn, recall_gb, recall_mlp, recall_torch, recall_dt, recall_cnn, recall_rnn],\n",
    "    \"F1 Score\": [f1, f1_rf, f1_svm, f1_knn, f1_gb, f1_mlp, f1_torch, f1_dt, f1_cnn, f1_rnn]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Seleccionar el mejor modelo basado en una combinación de métricas\n",
    "best_model_index = results_df[['Accuracy', 'Precision', 'Recall', 'F1 Score']].mean(axis=1).idxmax()\n",
    "print(f\"The best model based on the combination of metrics is: {results_df.iloc[best_model_index]['Model']}\")\n",
    "\n",
    "# Mostrar todos los resultados\n",
    "print(\"Model performance comparison:\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
